---
# PyTorch GPU Test Job
# Tests PyTorch with CUDA support
apiVersion: batch/v1
kind: Job
metadata:
  name: pytorch-gpu-test
  namespace: default
spec:
  template:
    metadata:
      labels:
        app: pytorch-gpu-test
    spec:
      restartPolicy: Never
      containers:
      - name: pytorch
        image: pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime
        command:
          - python3
          - -c
          - |
            import torch
            print("PyTorch version:", torch.__version__)
            print("CUDA available:", torch.cuda.is_available())
            print("CUDA version:", torch.version.cuda)
            print("cuDNN version:", torch.backends.cudnn.version())
            if torch.cuda.is_available():
                print("GPU count:", torch.cuda.device_count())
                print("Current GPU:", torch.cuda.current_device())
                print("GPU name:", torch.cuda.get_device_name(0))
                print("GPU memory:", torch.cuda.get_device_properties(0).total_memory / 1024**3, "GB")
                # Simple tensor operation on GPU
                x = torch.rand(5, 3).cuda()
                print("Tensor on GPU:", x.device)
                print("Test successful!")
            else:
                print("ERROR: CUDA not available!")
                exit(1)
        resources:
          limits:
            nvidia.com/gpu: 1
            memory: "4Gi"
          requests:
            memory: "2Gi"
  backoffLimit: 2
---
# TensorFlow GPU Test Job
# Tests TensorFlow with GPU support
apiVersion: batch/v1
kind: Job
metadata:
  name: tensorflow-gpu-test
  namespace: default
spec:
  template:
    metadata:
      labels:
        app: tensorflow-gpu-test
    spec:
      restartPolicy: Never
      containers:
      - name: tensorflow
        image: tensorflow/tensorflow:2.14.0-gpu
        command:
          - python3
          - -c
          - |
            import tensorflow as tf
            print("TensorFlow version:", tf.__version__)
            print("GPU devices:", tf.config.list_physical_devices('GPU'))
            gpus = tf.config.list_physical_devices('GPU')
            if gpus:
                print("GPU count:", len(gpus))
                for gpu in gpus:
                    print("GPU:", gpu)
                # Simple operation on GPU
                with tf.device('/GPU:0'):
                    a = tf.constant([[1.0, 2.0], [3.0, 4.0]])
                    b = tf.constant([[1.0, 1.0], [0.0, 1.0]])
                    c = tf.matmul(a, b)
                    print("Matrix multiplication result:", c.numpy())
                print("Test successful!")
            else:
                print("ERROR: No GPU devices found!")
                exit(1)
        resources:
          limits:
            nvidia.com/gpu: 1
            memory: "4Gi"
          requests:
            memory: "2Gi"
  backoffLimit: 2
